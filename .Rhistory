# Create a frequency table of words
word_freq <- table(word_list)
word_freq <- sort(word_freq, decreasing = TRUE)
# Plot the word cloud
wordcloud(names(word_freq), freq = word_freq, min.freq = 10,
random.order = FALSE, colors = brewer.pal(8, "Dark2"))
library(wordcloud)
install.packages("RColorBrewer")
knitr::opts_chunk$set(echo = TRUE)
library(wordcloud)
library(RColorBrewer)
library(wordcloud)
library(RColorBrewer)
# Combine all rows into a single text string
all_text <- str_c(shuffled_data$text, shuffled_data$subject, shuffled_data$title, collapse = " ")
# Combine all rows into a single text string
all_text <- str_c(shuffled_data$text, shuffled_data$subject, shuffled_data$title, collapse = " ")
# Combine all rows into a single text string
library(stringr)
all_text <- str_c(shuffled_data$text, shuffled_data$subject, shuffled_data$title, collapse = " ")
# Split the text into individual words
word_list <- unlist(strsplit(all_text, " "))
# Create a frequency table of words
word_freq <- table(word_list)
word_freq <- sort(word_freq, decreasing = TRUE)
# Plot the word cloud
wordcloud(names(word_freq), freq = word_freq, min.freq = 10,
random.order = FALSE, colors = brewer.pal(8, "Dark2"))
words_to_remove <- c("said", "president", "us","president","s","trump","will","new","states","t")
# Create pattern and remove words
pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"),")\\b")
# Apply the pattern-based removal to the specified columns
shuffled_data <- shuffled_data %>%
mutate(
title = str_replace_all(title, pattern, ""),
subject = str_replace_all(subject, pattern, ""),
text = str_replace_all(text, pattern, "")
)
words_to_remove <- c("said", "president", "us","president","s","trump","will","new","states","t")
# Create pattern and remove words
pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"),")\\b")
# Apply the pattern-based removal to the specified columns
shuffled_data <- shuffled_data %>%
mutate(
title = str_replace_all(title, pattern, ""),
subject = str_replace_all(subject, pattern, ""),
text = str_replace_all(text, pattern, "")
)
library(dplyr)
words_to_remove <- c("said", "president", "us","president","s","trump","will","new","states","t")
# Create pattern and remove words
pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"),")\\b")
# Apply the pattern-based removal to the specified columns
shuffled_data <- shuffled_data %>%
mutate(
title = str_replace_all(title, pattern, ""),
subject = str_replace_all(subject, pattern, ""),
text = str_replace_all(text, pattern, "")
)
# Combine columns into one
shuffled_data$combined <- paste(shuffled_data$title, shuffled_data$text, shuffled_data$subject, sep = " ")
# Process the combined column
split_combined <- strsplit(shuffled_data$combined, " ")
balanced_combined <- do.call(rbind, lapply(split_combined, function(x) {
length(x) <- max(lengths(split_combined))
return(x)
}))
balanced_combined <- as.data.frame(balanced_combined)
colnames(balanced_combined) <- paste0("word", 1:ncol(balanced_combined))
# Add the label column back
balanced_combined$type <- shuffled_data$type
head(balanced_combined)
# Reorder columns by moving 'type' to the first position
balanced_combined <- balanced_combined[c("type", setdiff(colnames(balanced_combined), "type"))]
# Check the new column order
colnames(balanced_combined)
library(arules)
library(dplyr)
# Step 1: Clean and prepare the transactions list
transactions_list <- apply(balanced_combined[, -ncol(balanced_combined)], 1, function(x) x[!is.na(x) & x != ""])
# Step 2: Remove empty transactions (this ensures there are no rows with no items)
transactions_list <- transactions_list[sapply(transactions_list, length) > 0]
# Step 3: Remove duplicates within each transaction (ensure no repeated words in a single transaction)
transactions_list <- lapply(transactions_list, function(x) unique(x))
# Step 4: Optionally, sort each transaction's items (for consistency)
transactions_list <- lapply(transactions_list, function(x) sort(x))
# Step 5: Convert the cleaned list to a transaction format using 'arules'
transactions <- as(transactions_list, "transactions")
# Step 6: summary the transactions object
summary(transactions)
par(las = 2)
itemFrequencyPlot(transactions, support = 0.5)
par(las = 2)
itemFrequencyPlot(transactions, support = 0.1)
par(las = 2)
itemFrequencyPlot(transactions, support = 0.2)
par(las = 2)
itemFrequencyPlot(transactions, support = 0.3)
par(las = 2)
itemFrequencyPlot(transactions, support = 0.35)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tm)
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(arules)
library(tidyverse)
true_data <- read.csv("True.csv")
fake_data <- read.csv("Fake.csv")
head(true_data)
columns_to_keep <- c("title", "text", "subject", "type")
fake_data<- fake_data %>% select(all_of(columns_to_keep))
colnames(fake_data)
true_data <- true_data %>% select(-date)
colnames(fake_data)
dim(true_data)
dim(fake_data)
library(tm)
library(SnowballC)
library(stringr)
# Function to clean text
library(SnowballC)  # For stemming
library(tm) # For stopwords
library(dplyr)
clean_text <- function(text) {
# Replace email addresses with 'emailaddr' (modified pattern)
text <- gsub('[a-zA-Z0-9\\-\\.]+@[a-zA-Z0-9]+\\.[a-zA-Z]{2,4}', 'emailaddr', text)
# Replace URLs with 'httpaddr'
text <- gsub('(http[s]?\\S+)|(\\w+\\.\\w{2,4}\\S*)', 'httpaddr', text)
# Replace currency symbols with 'moneysymb'
text <- gsub('Â£|\\$', 'moneysymb', text)
# Replace phone numbers with 'phonenumbr'
text <- gsub('\\+?\\(?\\d{1,2}\\)?[-\\s.]?\\d{3}[-\\s.]?\\d{3}[-\\s.]?\\d{4}', 'phonenumbr', text)
# Replace numbers with 'numbr'
text <- gsub('\\d+(\\.\\d+)?', 'numbr', text)
# Remove any non-word characters (except space)
text <- gsub('[^a-zA-Z0-9\\s]', ' ', text)
# Remove extra spaces and trim leading/trailing whitespaces
text <- gsub('\\s+', ' ', text)
text <- gsub('^\\s+|\\s+?$', '', text)
# Convert to lowercase
text <- tolower(text)
# Tokenize the text, remove stopwords, and apply stemming
words <- strsplit(text, " ")[[1]]
words <- words[!words %in% stopwords("en")]  # Remove stopwords
# Remove single-character words (like 'a', 'b', etc.)
words <- words[nchar(words) > 1]
# Apply stemming
words <- wordStem(words)
# Return the cleaned text as a single string
return(paste(words, collapse = " "))
}
# Apply the clean_text function to both 'title' and 'text' columns
true_data$title <- sapply(true_data$title, clean_text)
true_data$text <- sapply(true_data$text, clean_text)
fake_data$title <- sapply(fake_data$title, clean_text)
par(las = 2)
itemFrequencyPlot(transactions, support = 0.35)
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
ggplot(filtered_data, aes(x = word, y = count, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Word Occurrences by Category (true/fake)",
x = "Word",
y = "Frequency",
fill = "Category") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
ggplot(filtered_data, aes(x = reorder(word, -count), y = count, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Frequency of Words in True vs Fake News",
x = "Word",
y = "Frequency",
fill = "News Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
) +
scale_fill_brewer(palette = "Set1")
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
# Ensure 'type' is a factor
filtered_data$type <- as.factor(filtered_data$type)
# Create the plot
ggplot(filtered_data, aes(x = reorder(word, -count), y = count, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Frequency of Words in True vs Fake News",
x = "Word",
y = "Frequency",
fill = "News Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
) +
scale_fill_brewer(palette = "Set1")
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
# Ensure 'type' is a factor
filtered_data$type <- as.factor(filtered_data$type)
filtered_data <- filtered_data %>%
group_by(word) %>%
mutate(percentage = (count / sum(count)) * 100)
# Create the plot
ggplot(filtered_data, aes(x = reorder(word, -count), y = count, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Frequency of Words in True vs Fake News",
x = "Word",
y = "Frequency",
fill = "News Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
) +
scale_fill_brewer(palette = "Set1")
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
# Ensure 'type' is a factor
filtered_data$type <- as.factor(filtered_data$type)
# Create the plot
# Calculate the percentage for each word within its category
filtered_data <- filtered_data %>%
group_by(word) %>%
mutate(percentage = (count / sum(count)) * 100)
# Create the plot with percentages
ggplot(filtered_data, aes(x = reorder(word, -percentage), y = percentage, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Percentage of Words in True vs Fake News",
x = "Word",
y = "Percentage (%)",
fill = "News Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
) +
scale_fill_brewer(palette = "Set1")
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
# Ensure 'type' is a factor
filtered_data$type <- as.factor(filtered_data$type)
# Create the plot
# Calculate the percentage for each word within its category
filtered_data <- filtered_data %>%
group_by(word) %>%
mutate(percentage = (count / sum(count)) * 100)
# Create the plot with percentages
ggplot(filtered_data, aes(x = reorder(word, -percentage), y = percentage, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Percentage of Words in True vs Fake News",
x = "Word",
y = "Percentage (%)",
fill = "News Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
) +
scale_fill_manual(values = c("fake" = "red", "true" = "blue"),
breaks = c(0, 1),
labels = c("fake", "true"))
common_words = c('also','call','donald','like','peopl','presid','reuter','sai','time','year')
long_data <- balanced_combined %>%
pivot_longer(cols = starts_with("word"),
names_to = "word_column",
values_to = "word")
# Filter rows where "word" matches the target words
filtered_data <- long_data %>%
filter(word %in% common_words) %>%
group_by(word, type) %>%
summarise(count = n(), .groups = "drop")
# Create a bar chart
# Ensure 'type' is a factor
filtered_data$type <- as.factor(filtered_data$type)
# Create the plot
# Calculate the percentage for each word within its category
filtered_data <- filtered_data %>%
group_by(word) %>%
mutate(percentage = (count / sum(count)) * 100)
# Create the plot with percentages
ggplot(filtered_data, aes(x = reorder(word, -percentage), y = percentage, fill = type)) +
geom_bar(stat = "identity", position = "dodge") +
labs(
title = "Percentage of Words in True vs Fake News",
x = "Word",
y = "Percentage (%)",
fill = "News Type"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
axis.text = element_text(size = 10),
axis.title = element_text(size = 12)
) +
scale_fill_brewer(palette = "Set1")
textrules <- apriori(transactions, parameter = list(support =
0.01, confidence = 0.25, minlen = 2))
textrules
textrules <- apriori(transactions, parameter = list(support =
0.01, confidence = 0.25, minlen = 2))
textrules
summary(textrules)
inspect(textrules[1:20])
summary(textrules)
inspect(head(textrules, 20))
library(arules)
inspect(head(textrules, 20))
library(arules)
inspect(head(textrules, 20))
install.packages("arules")
library(arules)
inspect(head(textrules, 20))
knitr::opts_chunk$set(echo = TRUE)
library(arules)
inspect(head(textrules, 20))
knitr::opts_chunk$set(echo = TRUE)
install.packages("kernlab")
data("spirals")
data("spirals")
data("spirals")
data("spirals")
install.packages("kernlab")
install.packages("kernlab")
data("spirals")
install.packages("kernlab")
#install.packages("kernlab")
library(kernlab)
data("spirals")
data("spirals")
data("spirals")
plot("spirals")
data("spirals")
sc=specc(spirals,2)
plot(split.screen())
#install.packages("kernlab")
library(kernlab)
data("spirals")
sc=specc(spirals,2)
plot(split.screen())
install.packages("kernlab")
library(kernlab)
data("spirals")
sc=specc(spirals,2)
knitr::opts_chunk$set(echo = TRUE)
data("spirals")
sc=specc(spirals,2)
install.packages("kernlab")
library(kernlab)
#install.packages("kernlab")
library(kernlab)
#install.packages("kernlab")
library(kernlab)
data("spirals")
sc=specc(spirals,2)
plot(split.screen())
data("spirals")
sc=specc(spirals,5)
plot(split.screen())
data("spirals")
sc=specc(spirals,1)
data("spirals")
sc=specc(spirals,2)
plot(split.screen())
data("spirals")
sc=specc(spirals,2)
plot(split.screen())
# Load the 'spirals' dataset
data("spirals")
# Perform spectral clustering
sc <- specc(spirals, centers = 2)
# Plot the results
plot(spirals, col = sc@.Data, pch = 19, main = "Spectral Clustering on Spirals")
data("spirals")
# Perform spectral clustering
sc <- specc(spirals, centers = 3)
# Plot the results
plot(spirals, col = sc@.Data, pch = 19, main = "Spectral Clustering on Spirals")
data("spirals")
# Perform spectral clustering
sc <- specc(spirals, centers = 2)
# Plot the results
plot(spirals, col = sc@.Data, pch = 19, main = "Spectral Clustering on Spirals")
data("spirals")
# Perform spectral clustering
sc <- specc(spirals, centers = 2)
# Plot the results
plot(spirals, col = scx, pch = sc)
data("spirals")
# Perform spectral clustering
sc <- specc(spirals, centers = 2)
# Plot the results
plot(spirals, col = sc, pch = sc)
{r}
## (example for gaussian) create 3 clusters artificial to then simulate them with clustering techniques
## cluster c1
x1=rnorm(n=100,sd=1,mean=0)
x2=rnorm(n=100,sd=1,mean=0)
c1=cbind(x1,x2)
## cluster c2
x1=rnorm(n=100,sd=0.5,mean=3)
x2=rnorm(n=100,sd=0.5,mean=3)
c2=cbind(x1,x2)
## cluster c3
x1=rnorm(n=100,sd=0.5,mean=0)
x2=rnorm(n=100,sd=1,mean=-5)
c3=cbind(x1,x2)
## relerence simulated data (to sperate clusters we change the mean and standard deviation)
x=rbind(c1,c2,c3) ##rowbind lasa9hom l baadhhom lclusters
## scatter plot of x
plot(x)
scx= specc(x,centers=3)
plot(x, col=scx, pch=scx, cex=3)
scx= specc(x,centers=3)
plot(x, col=scx, pch=scx, cex=2)
scx= specc(x,centers=3)
plot(x, col=scx, pch=scx, cex=4)
scx= specc(x,centers=3)
plot(x, col=scx, pch=scx, cex=3)
DS=dis(spirals)
DS=dist(spirals)
hs=hclust(DS,method = 'complete')
plot(hs, main = "Hierarchical Clustering Dendrogram", xlab = "Data Points", ylab = "Height")
